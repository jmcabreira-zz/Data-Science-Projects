{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 400)\n",
    "import pickle\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "#azdias = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "#customers = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the azdias dataframe to a pickle object \n",
    "#pickle.dump(azdias, open(\"azdias.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload azdias object \n",
    "azdias = pickle.load(open(\"azdias.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from helpers import save_csv\n",
    "#helpers.save_csv(azdias, 'data_project', 'Udacity_AZDIAS_052018.csv')\n",
    "#helpers.save_csv(customers, 'data_project', 'Udacity_CUSTOMERS_052018.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0.1 Data exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#azdias = pd.read_csv('./data_project/Udacity_AZDIAS_052018.csv')\n",
    "#azdias.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "#azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers = pd.read_csv('./data_project/Udacity_CUSTOMERS_052018.csv')\n",
    "#customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Describe Customers :\\n')\n",
    "display(customers.describe())\n",
    "\n",
    "print('Describe Population :\\n')\n",
    "display(azdias.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of dataframes\n",
    "print('Shape of azdias dataframe: {}\\n'.format(azdias.shape))\n",
    "print('Shape of customers dataframe: {}\\n'.format(customers.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check if azdias contains all columns in customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if azdias contains all columns in customers\n",
    "\n",
    "customers_columns_list = list(customers.columns)\n",
    "azdias_columns_list = list(azdias.columns)\n",
    "\n",
    "result =  all(elem in customers_columns_list  for elem in azdias_columns_list)\n",
    " \n",
    "if result:\n",
    "    print(\"Yes, customers dataframe contains all columns in azdias dataframe\")    \n",
    "else :\n",
    "    print(\"No, customers dataframe does not contain all columns in azdias dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data_info and Mising values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will explore a little deeper the data. Let's load the data_info.csv file so that we will have more information about the columns. This would help us in our data cleaning processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = pd.read_csv('data_info.csv')\n",
    "data_info.drop(['Attribute','ISBLANK'], axis = 1, inplace = True)\n",
    "data_info.rename(columns={\"Attribute.1\": \"Attribute\", \"Missing_value\": \"Missing_Value\"}, inplace = True)\n",
    "\n",
    "data_info.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_info.Attribute.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique code for missing values\n",
    "data_info.Missing_Value.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1  Check for Missing Values  - azdias df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that we have 273 columns with missing data. We can also see below that even before the missing data mapping process, we have few columns with high percentage of missing values, such as ALTER_KIND4 (99.86%), ALTER_KIND3 (99.31%), ALTER_KIND2 (96.69%),ALTER_KIND1 (90.90%), EXTSEL992(73.40%), KK_KUNDENTYP (65.60%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with missing values\n",
    "missing_data = azdias.isnull().sum()[azdias.isnull().sum() !=0]\n",
    "\n",
    "# dictionary with missing values and their respective percentage \n",
    "\n",
    "total_rows = azdias.shape[0]\n",
    "missing_dict = {'Missing_Count': missing_data.values, \n",
    "                'Percentage': np.round(missing_data.values*100/(total_rows),2)}\n",
    "\n",
    "\n",
    "#Create DataFrame\n",
    "azdias_missing_initial = pd.DataFrame(missing_dict, index = missing_data.index)\n",
    "azdias_missing_initial.sort_values(by = 'Missing_Count', ascending = False, inplace = True)\n",
    "print('Number of columns with missing values: ', azdias_missing_initial.shape[0])\n",
    "display('     azdias_missing_initial DataFrame: ')\n",
    "azdias_missing_initial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-check\n",
    "print('Percentage of missing values (ALTER_KIND4): ', round(100*azdias.isnull().sum()['ALTER_KIND4']/ total_rows,2))\n",
    "print('Percentage of missing values (ALTERSKATEGORIE_FEIN): ',round(100*azdias.isnull().sum()['ALTERSKATEGORIE_FEIN']/ total_rows,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe with attributes and the missing codes from data_info.csv file : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_code_df = data_info.drop(['Description','Value','Meaning'], axis = 1)\n",
    "#missing_code_df.set_index('Attribute', inplace = True)\n",
    "\n",
    "#missing_code_df['Missing_value'] = missing_code_df['Missing_value'].str.replace('[','').replace(']','').split(',').values\n",
    "\n",
    "\n",
    "display('Missing codes and dtype: ',missing_code_df.Missing_Value.unique())\n",
    "print()\n",
    "missing_code_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_code_df.Attribute.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_code_df.drop_duplicates(keep='first',inplace=True) \n",
    "display(len(missing_code_df.Attribute.unique()))\n",
    "\n",
    "missing_code_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_code_df.Attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_code_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables are not categorical, and have no missing value code\n",
    "# let's give them a -1 code so that we can use the built in function eval() to build our dictionary with int codes\n",
    "missing_code_df.fillna('[-1]', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with attribute as key and its respective missing code as value\n",
    "from helpers import create_missing_code_dict\n",
    "missing_dict = create_missing_code_dict(missing_code_df)\n",
    "#missing_dict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that there are attributes in the azdias dataframe that are missing in the data_info dataframe. To be more specific, we have  93 missing Attributes and because of that we don't have information regarding their missing code.So, I will only consider nan as missing value for those attributes( I could try to consider 0 as missing value but I have no information regarding those features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_column_count =0\n",
    "present_column_count = 0\n",
    "missing_column_list = []\n",
    "\n",
    "for col in azdias.columns[1:]:\n",
    "    if col not in missing_dict:\n",
    "        missing_column_list.append(col)\n",
    "        missing_column_count += 1\n",
    "        pass\n",
    "    else:\n",
    "        present_column_count +=1\n",
    "print('Number of columns of azdias df that are missing in data_info df: ', missing_column_count)\n",
    "print('Number of columns of azdias df contained in data_info df: ', present_column_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number missing columns that have 0 (potencial missing value)\n",
    "# NOTE : I am not considering 0 as missing value. This is just a test\n",
    "zero_one_missing_code = []\n",
    "for col in missing_column_list:\n",
    "   # for val in azdias[col].value_counts().index:\n",
    "        if 0 in azdias[col].value_counts().index:\n",
    "           zero_one_missing_code.append(col)\n",
    "print('Number of columns that contains 0: ',len(zero_one_missing_code))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import valid_values_dict\n",
    "valid_values_dict = valid_values_dict(azdias, missing_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can see below that the missing codes in the azdias dataframe has been converted to nan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataframe with missing codes converted to nan\n",
    "azdias_after_encoding = azdias.copy()\n",
    "\n",
    "\n",
    "# skip LNR column\n",
    "for col in azdias.columns[1:]:  \n",
    "\n",
    "    azdias_after_encoding[col] = azdias_after_encoding[col].map(valid_values_dict[col])\n",
    "\n",
    "azdias_after_encoding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total of missing values per columns after converting codes\n",
    "total_nan_after_encoding = azdias_after_encoding.isnull().sum()\n",
    "# total of missing values per columns before converting codes\n",
    "total_nan_before_encoding = azdias.isnull().sum()\n",
    "\n",
    "#difference\n",
    "\n",
    "diff = total_nan_after_encoding - total_nan_before_encoding\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values before encoding:', total_nan_before_encoding.sum())\n",
    "print('Missing values after encoding:', total_nan_after_encoding.sum())\n",
    "print('Total increase after encoding:', diff.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we can work on the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the azdias_after_encoding dataframe to a pickle object since it takes up so much room in memory.\n",
    "#pickle.dump(azdias_after_encoding, open(\"azdias_after_encoding.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#azdias_after_encoding  = pickle.load(open(\"azdias_after_encoding.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dict = {'Before_Encoding': total_nan_before_encoding[1:],\n",
    "               'After_Enconding': total_nan_after_encoding[1:],\n",
    "               'Difference':diff[1:], \n",
    "               'Percent_nan': 100* total_nan_after_encoding/ azdias.shape[0]}\n",
    "\n",
    "missing_df = pd.DataFrame(missing_dict)\n",
    "missing_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import missing_values_barplt\n",
    "missing_values_barplt(missing_df,'Before_Encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_barplt(missing_df,'After_Enconding')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_barplt(missing_df,'Difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import missing_values_barplt\n",
    "missing_values_barplt(missing_df,'Percent_nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plots above show us that there are attributes with a lot of missing values after converting the missing value codes to nan variables. It's important to notice that some columns got a significant increase in missing values. Besides, the column AGER_TYP that has no missing values before the encoding, got tons of nan variables.\n",
    "\n",
    "I will now check the distribution of the some attributes in order to find potencial outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_more_than_40_pct = missing_df.loc[missing_df.Percent_nan >= 40, :].sort_values(by = ['Percent_nan'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_more_than_40_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values histogram\n",
    "from helpers import hist_missing_values\n",
    "hist_missing_values(missing_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values < 30 %\n",
    "hist_missing_values(missing_df,threshold = 30,greater_or_less ='less' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values > 30%\n",
    "hist_missing_values(missing_df,threshold = 40,greater_or_less ='greater' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = [10,20,30,40,50,50,60]\n",
    "print('Missing Values Per Features:')\n",
    "print()\n",
    "for i in threshold:\n",
    "    outliers = missing_df[missing_df['Percent_nan'] > i]\n",
    "    print('Features with more than {}% of missing values: {}'.format(i,outliers.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_after_encoding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions above show us that most of the features of the dataset have zero or small amount of missing values. In addition to that, we can notice that 35 featues have more than 40% of missing values. It seems reasonable to delete those columns since those features with high amount of missing values do note give us much information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_more_than_40_pct.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features with more than 40% missing values:',missing_more_than_40_pct.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_after_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_more_than_40_pct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataframe before the columns removal:',azdias_after_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [x for x in missing_more_than_40_pct.index]\n",
    "azdias_dropped_features = azdias_after_encoding.drop(columns_to_drop, axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataframe after the columns removal:',azdias_dropped_features.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution with number of missing values per row after the outlier columns removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7))\n",
    "azdias_dropped_features.isnull().sum(axis=1).hist(bins=20)\n",
    "plt.title('Missing Values per Row');\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Number of Rows');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of missing value for rows with about 240 nan variables\n",
    "round(250/azdias_dropped_features.shape[1],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider the number of missing values per row. Looking at the distribution above, we can see that there are rows with more than 250 missing values. This represents about 76% of nan varibles in the rows. So, in order to analyse those rows I will split the dataframe into two. The first will have less missing values than a specified threshold and the second will have the number of missing values greater than this threshold. Than, I will analyse the distribution of those groups and check if they are too different from one another. If so,I will be comfortable dropping those rows from my dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe \n",
    "df_new = azdias_dropped_features.copy()\n",
    "\n",
    "df_low_missing_values = df_new.dropna(thresh= 250) # Keep only the rows with at least 250 non-NA values\n",
    "\n",
    "df_high_missing = df_new[~df_new.index.isin(df_low_missing_values.index)]\n",
    "\n",
    "df_low_missing_values.shape, df_high_missing.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_df_plot(df_low_missing, df_high_missing, column_names):\n",
    "    \n",
    "    for column_name in column_names:\n",
    "    \n",
    "        fig = plt.figure(figsize = (15,5))\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        ax1.title.set_text('Low Missing Values')\n",
    "        sns.countplot(df_low_missing.loc[:, column_name])\n",
    "\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        ax2.title.set_text('High Missing Values')\n",
    "        sns.countplot(df_high_missing.loc[:,column_name])      \n",
    "        \n",
    "        #fig.subtitle(column_name)\n",
    "        plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ANREDE_KZ','FINANZTYP','ZABEOTYP', 'SEMIO_TRADV', 'FINANZ_VORSORGER', 'FINANZTYP', 'ALTERSKATEGORIE_GROB', 'ONLINE_AFFINITAET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_df_plot(df_low_missing_values , df_high_missing ,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#sampling with replacement\n",
    "\n",
    "sampling = random.choices(azdias_dropped_features.columns, k=15)\n",
    "print(\"Randomly selected multiple columns: \\n \", sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_df_plot(df_low_missing_values , df_high_missing ,sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that some features  have distribution completely different from the dataframe with smaller amount of missing value.This suggests that these two groups are different from one another.Based on that, from now on, I will keep working with the low_missing_values data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unsupervised learning algorithm that will be used to build the customer segmentation, requires numerical values. Because of that, all the data must be numeric encoded so that the model can proceed the way it is supposed to proceed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop LNR columns - it looks to be just an identifier \n",
    "#df_low_missing_values.drop(['LNR'] ,axis =1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dictionaries contains the binary features and the multiple features. This helps to identify the columns that will need to be better investigated so that we can encode them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values_dict(df):\n",
    "    \n",
    "    binary_variable = {}\n",
    "    multiple_categorical_feature = {}\n",
    "\n",
    "    for column in df.columns:\n",
    "\n",
    "        unique_values = df[column].nunique()\n",
    "\n",
    "        if unique_values <= 2:\n",
    "\n",
    "            binary_variable[column] = unique_values\n",
    "\n",
    "        else:\n",
    "            multiple_categorical_feature[column] = unique_values\n",
    "\n",
    "    return binary_variable, multiple_categorical_feature\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_variable , multiple_categorical_feature = unique_values_dict(df_low_missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('============ BINARY FEATURES: =============\\n')\n",
    "print('\\n'.join('column: {} Unique Values: {}'.format(key,value) for key,value in binary_variable.items()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low_missing_values.OST_WEST_KZ.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only benary feature that needs to be encoded is OST_WEST_KZ\n",
    "* array(['W', 'O'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('========= MULTIPLE VALUES FEATURES: ==========\\n')\n",
    "print('\\n'.join('column: {} Unique Values: {}'.format(key,value) for key,\n",
    "                value in multiple_categorical_feature.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.sort(df_low_missing_values.AKT_DAT_KL.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object features \n",
    "obj_features = df_low_missing_values.select_dtypes(include = ['object']).columns\n",
    "obj_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values fperfeature\n",
    "for k,v in multiple_categorical_feature.items():\n",
    "    \n",
    "    print(k,'\\n','Unique value :', end=' ')\n",
    "    \n",
    "    if k in list(obj_features): # skip object type features \n",
    "        pass\n",
    "    else:\n",
    "        print(np.sort(df_low_missing_values[k].unique()))\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing each feature and its unique values, we can see that most of the features are already encoded.Even though there are ordinal and nominal variables in the dataset, it seems that categorizing them as been of the same type (categorical) will not cause huge inpact on the model. Becaus of that, I will assume all variables as being categorical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object features \n",
    "obj_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After analysing all of those features, I decided to do as follows:**\n",
    "* **OST_WEST_KZ** binary feature that will bee re-encoded to 0 and 1\n",
    "* LNR - It seems to be a index so we can get rid of it\n",
    "* **CAMEO_DEU_2015** - This feature has many rows with 'XX' category (347 rows). I will consider those inputs as missing values and replace with a numerical value\n",
    "* **CAMEO_DEUG_2015** - Similarly the CAMEO_INTL_2015, this feature has many 'X' inputs. I will do the same as the above feature\n",
    "* **CAMEO_INTL_2015** - Same as above feature\n",
    "* **D19_LETZTER_KAUF_BRANCHE** - This featur has 36 categories (strings) and needs to be encoded\n",
    "* **EINGEFUEGT_AM** - It is a date format feature and needs to be re-enconded (year e/or month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-d85fc583f1a6>, line 118)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-d85fc583f1a6>\"\u001b[0;36m, line \u001b[0;32m118\u001b[0m\n\u001b[0;31m    df_dummies[column] = 0.0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from helpers import create_missing_code_dict\n",
    "from helpers import valid_values_dict\n",
    "def clean_df(df, missing_code_df, customer_data = False):\n",
    "    \n",
    "    print('====== Delete CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP features if customer_df ====')\n",
    "    \n",
    "    if customer_data:\n",
    "        df.drop(['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP'], axis = 1, inplace = True)\n",
    "        \n",
    "    \n",
    "    print('====== Drop index LNR ====== ')\n",
    "    \n",
    "    df.drop(['LNR'], axis = 1, inplace = True)\n",
    "    \n",
    "    print('====== Converte Missing Code ======')\n",
    "    \n",
    "\n",
    "    missing_dict = create_missing_code_dict(missing_code_df)\n",
    "    \n",
    "  \n",
    "    valid_values_dict_ = valid_values_dict(df, missing_dict)\n",
    "    \n",
    "    # Dataframe with missing codes converted to nan\n",
    "    #df_copy = df.copy()\n",
    "\n",
    "   \n",
    "    for col in df.columns: \n",
    "\n",
    "        df[col] = df[col].map(valid_values_dict_[col])\n",
    "        \n",
    "    print('====== Drop Features with more than 40% of missing values=====')\n",
    "    \n",
    "    columns_to_drop = ['ALTER_KIND4',\n",
    "                         'TITEL_KZ',\n",
    "                         'ALTER_KIND3',\n",
    "                         'D19_TELKO_ONLINE_DATUM',\n",
    "                         'D19_BANKEN_OFFLINE_DATUM',\n",
    "                         'ALTER_KIND2',\n",
    "                         'D19_TELKO_ANZ_12',\n",
    "                         'D19_BANKEN_ONLINE_QUOTE_12',\n",
    "                         'D19_BANKEN_ANZ_12',\n",
    "                         'D19_TELKO_ANZ_24',\n",
    "                         'D19_VERSI_ANZ_12',\n",
    "                         'D19_TELKO_OFFLINE_DATUM',\n",
    "                         'ALTER_KIND1',\n",
    "                         'D19_BANKEN_ANZ_24',\n",
    "                         'D19_VERSI_ANZ_24',\n",
    "                         'D19_BANKEN_ONLINE_DATUM',\n",
    "                         'GREEN_AVANTGARDE',\n",
    "                         'D19_BANKEN_DATUM',\n",
    "                         'AGER_TYP',\n",
    "                         'D19_VERSAND_ONLINE_QUOTE_12',\n",
    "                         'D19_TELKO_DATUM',\n",
    "                         'EXTSEL992',\n",
    "                         'D19_GESAMT_ONLINE_QUOTE_12',\n",
    "                         'D19_VERSAND_ANZ_12',\n",
    "                         'D19_VERSAND_OFFLINE_DATUM',\n",
    "                         'D19_GESAMT_ANZ_12',\n",
    "                         'KK_KUNDENTYP',\n",
    "                         'D19_VERSAND_ANZ_24',\n",
    "                         'D19_GESAMT_OFFLINE_DATUM',\n",
    "                         'D19_KONSUMTYP',\n",
    "                         'D19_GESAMT_ANZ_24',\n",
    "                         'D19_VERSAND_ONLINE_DATUM',\n",
    "                         'KBA05_BAUMAX',\n",
    "                         'D19_GESAMT_ONLINE_DATUM',\n",
    "                         'D19_VERSAND_DATUM']\n",
    "\n",
    "    \n",
    "    #df_parsed = df.copy()\n",
    "    df.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    print('====== Delete Columns ====')\n",
    "    \n",
    "    # Split dataframe \n",
    "    \n",
    "    #df_copy = df_parsed.copy()\n",
    "    df = df.dropna(thresh= 250) # Keep only the rows with at least 250 non-NA values\n",
    "   \n",
    "    \n",
    "    print('======== Impute the missing values ======= ')\n",
    "    \n",
    "    \n",
    "    df_most_freq_values_imputed = impute_values(df_parsed)\n",
    "    \n",
    "    \n",
    "    print('====== Re-encode binary fature (OST_WEST_KZ) ======')\n",
    "    \n",
    "    bin_values = {'W': 1, 'O':0}\n",
    "    df_most_freq_values_imputed['OST_WEST_KZ'] = df_most_freq_values_imputed['OST_WEST_KZ'].map(bin_values)\n",
    "    \n",
    "    print('======= Re-encode multi categorical features ========')\n",
    "    \n",
    "    to_reencode = ['CAMEO_DEU_2015',\n",
    "                   'CAMEO_DEUG_2015',\n",
    "                   'CAMEO_INTL_2015',\n",
    "                   'D19_LETZTER_KAUF_BRANCHE']\n",
    "    \n",
    "    \n",
    "    df_dummies = pd.get_dummies(df_most_freq_values_imputed, columns = to_reencode)\n",
    "    \n",
    "    print('======== Re-encode EINGEFUEGT_AM to year and month ======= ')\n",
    "    \n",
    "    df_dummies['EINGEFUEGT_AM'] = pd.to_datetime( df_dummies['EINGEFUEGT_AM'],\n",
    "                                                 format = '%Y/%m/%d' )\n",
    "                                                 \n",
    "    df_dummies['EINGEFUEGT_AM'] = df_dummies['EINGEFUEGT_AM'].dt.year\n",
    "    df_dummies['EINGEFUEGT_AM_month'] = df_dummies['EINGEFUEGT_AM'].dt.month\n",
    "                                                 \n",
    "    \n",
    "    if columns is not None:\n",
    "        \n",
    "        diff = np.setdiff1d(columns, df_dummies.columns)\n",
    "        #for column in diff:\n",
    "        print(' Missing column:',diff)\n",
    "\n",
    "            df_dummies[column] = 0.0\n",
    "            df_dummies[column] = df_dummies[column].astype('float')\n",
    "\n",
    "    \n",
    "    \n",
    "    return df_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_values(df):\n",
    "    \n",
    "    df_copy = df.copy\n",
    "    \n",
    "    columns = df_copy.columns[df_copy.isnull().sum()]\n",
    "    count = 0\n",
    "    \n",
    "    for column in columns:\n",
    "        count += 1\n",
    "        most_frequent_value = df_copy.groupby([column]).count().sort_values(ascending = False).index[0]\n",
    "        \n",
    "        df_copy[columns].fillna(most_frequent_value, inplace = True)\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After analysing all of those features, I decided to do as follows:**\n",
    "* **OST_WEST_KZ** binary feature that will bee re-encoded to 0 and 1\n",
    "* **LNR** - It seems to be a index so we can get rid of it\n",
    "* **CAMEO_DEU_2015** - This feature has many rows with 'XX' category (347 rows). I will consider those inputs as missing values and replace with a numerical value. The feature needs to be reencoded.\n",
    "* **CAMEO_DEUG_2015** - Similarly the CAMEO_INTL_2015, this feature has many 'X' inputs. I will do the same as the above feature\n",
    "* **CAMEO_INTL_2015** - Same as above feature\n",
    "* **D19_LETZTER_KAUF_BRANCHE** - This featur has 36 categories (strings) and needs to be encoded\n",
    "* **EINGEFUEGT_AM** - It is a date format feature and needs to be re-enconded (year e/or month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display(pd.to_datetime(df_low_missing_values.EINGEFUEGT_AM, format=\"%Y-%m-%d\").head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_low_missing_values['year'] = df_low_missing_values.EINGEFUEGT_AM.dt.year\n",
    "#df_low_missing_values['month'] = df_low_missing_values.EINGEFUEGT_AM.dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mailout_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_csv(mailout_train, 'data_project', 'Udacity_MAILOUT_052018_TRAIN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer â€“ this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_csv(mailout_test, 'data_project', 'Udacity_MAILOUT_052018_TEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mailout_train.shape )\n",
    "display(mailout_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train.CAMEO_INTL_2015.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
